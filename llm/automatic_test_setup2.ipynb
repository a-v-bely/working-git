{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ac1497-63c5-4675-830d-5c147bdb0773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:12:40.751609Z",
     "iopub.status.busy": "2024-11-27T10:12:40.751194Z",
     "iopub.status.idle": "2024-11-27T10:12:43.781667Z",
     "shell.execute_reply": "2024-11-27T10:12:43.780734Z",
     "shell.execute_reply.started": "2024-11-27T10:12:40.751585Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f746e6-dcc3-4d1b-bdb7-ed9a8b9b82c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:12:43.783435Z",
     "iopub.status.busy": "2024-11-27T10:12:43.782952Z",
     "iopub.status.idle": "2024-11-27T10:12:57.932050Z",
     "shell.execute_reply": "2024-11-27T10:12:57.931113Z",
     "shell.execute_reply.started": "2024-11-27T10:12:43.783413Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc279cf231b04f3ab40d0499419832cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# model_id = 'IlyaGusev/saiga_llama3_70b_sft_m1_d5_abliterated_awq_4bit' # + QUANTIZED\n",
    "# model_id = \"IlyaGusev/saiga_llama3_70b_sft_m1_d5_abliterated_kto_m1_d2\" # + FULL\n",
    "model_id = \"Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24\" # + FULL\n",
    "# model_id = 'IlyaGusev/saiga_nemo_12b' # +\n",
    "# model_id = 'IlyaGusev/saiga_llama3_8b' # +\n",
    "# model_id = 'msu-rcc-lair/RuadaptQwen2.5-32B-instruct' # \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='balanced'\n",
    "    ) \n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a23fbe-a9bb-4936-99c3-e04fd38641b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:12:57.933681Z",
     "iopub.status.busy": "2024-11-27T10:12:57.933105Z",
     "iopub.status.idle": "2024-11-27T10:12:58.557631Z",
     "shell.execute_reply": "2024-11-27T10:12:58.556727Z",
     "shell.execute_reply.started": "2024-11-27T10:12:57.933658Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349b3419-3a9a-45ae-8b3f-2d694dd40e7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:12:58.560045Z",
     "iopub.status.busy": "2024-11-27T10:12:58.559432Z",
     "iopub.status.idle": "2024-11-27T10:13:02.567915Z",
     "shell.execute_reply": "2024-11-27T10:13:02.566928Z",
     "shell.execute_reply.started": "2024-11-27T10:12:58.560022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDER = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86224a0-05f0-48ea-9a1b-ba8b32676752",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:02.569239Z",
     "iopub.status.busy": "2024-11-27T10:13:02.568872Z",
     "iopub.status.idle": "2024-11-27T10:13:02.633998Z",
     "shell.execute_reply": "2024-11-27T10:13:02.633136Z",
     "shell.execute_reply.started": "2024-11-27T10:13:02.569213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import notebook\n",
    "import time\n",
    "import pandas as pd\n",
    "from uuid import uuid4\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bda340d-204e-4107-ba12-f813e49026b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:02.635584Z",
     "iopub.status.busy": "2024-11-27T10:13:02.635257Z",
     "iopub.status.idle": "2024-11-27T10:13:03.304843Z",
     "shell.execute_reply": "2024-11-27T10:13:03.303897Z",
     "shell.execute_reply.started": "2024-11-27T10:13:02.635563Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PDFMinerLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55822ac-5131-42c8-87cf-c0e35cc3eabf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:03.306101Z",
     "iopub.status.busy": "2024-11-27T10:13:03.305756Z",
     "iopub.status.idle": "2024-11-27T10:13:03.352547Z",
     "shell.execute_reply": "2024-11-27T10:13:03.351390Z",
     "shell.execute_reply.started": "2024-11-27T10:13:03.306079Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7373a061-2729-4a5d-a03a-9fcdf93a79e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:03.353760Z",
     "iopub.status.busy": "2024-11-27T10:13:03.353423Z",
     "iopub.status.idle": "2024-11-27T10:13:03.424523Z",
     "shell.execute_reply": "2024-11-27T10:13:03.423650Z",
     "shell.execute_reply.started": "2024-11-27T10:13:03.353737Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PDFMinerLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07658822-1b06-412b-b371-09b3b5f69f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:03.425888Z",
     "iopub.status.busy": "2024-11-27T10:13:03.425478Z",
     "iopub.status.idle": "2024-11-27T10:13:03.477706Z",
     "shell.execute_reply": "2024-11-27T10:13:03.476867Z",
     "shell.execute_reply.started": "2024-11-27T10:13:03.425860Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_single_document(file_path: str) -> Document:\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADER_MAPPING:\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()[0]\n",
    "    else:\n",
    "        return Document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b89bcedf-59bf-4e70-a338-c2cb3508ddd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:03.480028Z",
     "iopub.status.busy": "2024-11-27T10:13:03.479686Z",
     "iopub.status.idle": "2024-11-27T10:13:03.522469Z",
     "shell.execute_reply": "2024-11-27T10:13:03.521674Z",
     "shell.execute_reply.started": "2024-11-27T10:13:03.480008Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if len(line.strip()) > 2]\n",
    "    text = \"\\n\".join(lines).strip()\n",
    "    if len(text) < 10:\n",
    "        return None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5325cf05-c7c6-44cf-8f65-a5d278cc4d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:03.523725Z",
     "iopub.status.busy": "2024-11-27T10:13:03.523149Z",
     "iopub.status.idle": "2024-11-27T10:13:03.544536Z",
     "shell.execute_reply": "2024-11-27T10:13:03.543733Z",
     "shell.execute_reply.started": "2024-11-27T10:13:03.523706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_files(file_paths):\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d91b553-f145-452e-a13b-9a3a56706e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:03.545698Z",
     "iopub.status.busy": "2024-11-27T10:13:03.545355Z",
     "iopub.status.idle": "2024-11-27T10:13:03.565090Z",
     "shell.execute_reply": "2024-11-27T10:13:03.564352Z",
     "shell.execute_reply.started": "2024-11-27T10:13:03.545678Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_index(file_paths, db, chunk_size, chunk_overlap, file_warning):\n",
    "    extensions = ['csv', 'doc', 'docx', 'enex', 'epub', 'html', 'md', 'odt', 'pdf', 'ppt', 'pptx', 'txt']\n",
    "    if any(fp.split('.')[-1] in extensions for fp in file_paths):\n",
    "        documents = [load_single_document(path) for path in file_paths]\n",
    "    else:\n",
    "        documents = [load_single_document(file_paths)]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    documents = text_splitter.split_documents(documents)\n",
    "    print(\"Documents after split:\", len(documents))\n",
    "    fixed_documents = []\n",
    "    for doc in documents:\n",
    "        doc.page_content = process_text(doc.page_content)\n",
    "        if not doc.page_content:\n",
    "            continue\n",
    "        fixed_documents.append(doc)\n",
    "    print(\"Documents after processing:\", len(fixed_documents))\n",
    "\n",
    "    texts = [doc.page_content for doc in fixed_documents]\n",
    "    embeddings = EMBEDDER.encode(texts, convert_to_tensor=True)\n",
    "    db = {\"docs\": texts, \"embeddings\": embeddings}\n",
    "    print(\"Embeddings calculated!\")\n",
    "    \n",
    "    file_warning = f\"Загружено {len(fixed_documents)} фрагментов! Можно задавать вопросы.\"\n",
    "    return db, file_warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5453aba2-5f96-49c7-bcdb-3a3e2aa1f309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:03.566200Z",
     "iopub.status.busy": "2024-11-27T10:13:03.565873Z",
     "iopub.status.idle": "2024-11-27T10:13:03.596405Z",
     "shell.execute_reply": "2024-11-27T10:13:03.595633Z",
     "shell.execute_reply.started": "2024-11-27T10:13:03.566180Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve(last_user_message, db, retrieved_docs, k_documents):\n",
    "    retrieved_docs = \"\"\n",
    "    query_embedding = EMBEDDER.encode(last_user_message, convert_to_tensor=True)\n",
    "    scores = cos_sim(query_embedding, db[\"embeddings\"])[0]\n",
    "    if len(scores) <= k_documents:\n",
    "        k_documents = len(scores)\n",
    "    top_k_idx = torch.topk(scores, k=k_documents)[1]\n",
    "    top_k_documents = [db[\"docs\"][idx] for idx in top_k_idx]\n",
    "    retrieved_docs = \"\\n\\n\".join(top_k_documents)\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c740ab5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:03.597476Z",
     "iopub.status.busy": "2024-11-27T10:13:03.597135Z",
     "iopub.status.idle": "2024-11-27T10:13:03.691584Z",
     "shell.execute_reply": "2024-11-27T10:13:03.690822Z",
     "shell.execute_reply.started": "2024-11-27T10:13:03.597457Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 250\n",
    "overlap = 50\n",
    "temperature = 0.5\n",
    "beam_search = 5\n",
    "top_k = 30\n",
    "k_documents = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7121806e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:03.692685Z",
     "iopub.status.busy": "2024-11-27T10:13:03.692357Z",
     "iopub.status.idle": "2024-11-27T10:13:04.866475Z",
     "shell.execute_reply": "2024-11-27T10:13:04.865567Z",
     "shell.execute_reply.started": "2024-11-27T10:13:03.692666Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents after split: 618\n",
      "Documents after processing: 615\n",
      "Embeddings calculated!\n"
     ]
    }
   ],
   "source": [
    "metamentor_files = [os.path.join('corpus', f) for f in os.listdir('corpus') if os.path.isfile(os.path.join('corpus', f))]\n",
    "metamentor_files = upload_files(metamentor_files)\n",
    "metamentor_database = []\n",
    "metamentor_corpus_index, metamentor_warn = build_index(metamentor_files, metamentor_database,  chunk_size, overlap, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8124ade1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:04.867748Z",
     "iopub.status.busy": "2024-11-27T10:13:04.867420Z",
     "iopub.status.idle": "2024-11-27T10:13:04.908200Z",
     "shell.execute_reply": "2024-11-27T10:13:04.907289Z",
     "shell.execute_reply.started": "2024-11-27T10:13:04.867726Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents after split: 28\n",
      "Documents after processing: 28\n",
      "Embeddings calculated!\n"
     ]
    }
   ],
   "source": [
    "spb_corpus = [os.path.join('spb_corpus', f) for f in os.listdir('spb_corpus') if os.path.isfile(os.path.join('spb_corpus', f))]\n",
    "spb_corpus = upload_files(spb_corpus)\n",
    "spb_database = []\n",
    "spb_corpus_index, spb_warn = build_index(spb_corpus, spb_database,  chunk_size, overlap, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e328e261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:04.909890Z",
     "iopub.status.busy": "2024-11-27T10:13:04.909078Z",
     "iopub.status.idle": "2024-11-27T10:13:05.194373Z",
     "shell.execute_reply": "2024-11-27T10:13:05.193491Z",
     "shell.execute_reply.started": "2024-11-27T10:13:04.909867Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents after split: 264\n",
      "Documents after processing: 264\n",
      "Embeddings calculated!\n"
     ]
    }
   ],
   "source": [
    "svo_corpus = [os.path.join('svo_corpus', f) for f in os.listdir('svo_corpus') if os.path.isfile(os.path.join('svo_corpus', f))]\n",
    "svo_corpus = upload_files(svo_corpus)\n",
    "svo_database = []\n",
    "svo_corpus_index, svo_warn = build_index(svo_corpus, svo_database,  chunk_size, overlap, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43fcdf91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:05.195641Z",
     "iopub.status.busy": "2024-11-27T10:13:05.195315Z",
     "iopub.status.idle": "2024-11-27T10:13:06.154295Z",
     "shell.execute_reply": "2024-11-27T10:13:06.153375Z",
     "shell.execute_reply.started": "2024-11-27T10:13:05.195620Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents after split: 264\n",
      "Documents after processing: 264\n",
      "Embeddings calculated!\n"
     ]
    }
   ],
   "source": [
    "bumaga_corpus = [os.path.join('bumaga', f) for f in os.listdir('bumaga') if os.path.isfile(os.path.join('bumaga', f))]\n",
    "bumaga_corpus = upload_files(svo_corpus)\n",
    "bumaga_database = []\n",
    "bumaga_corpus_index, bumaga_warn = build_index(bumaga_corpus, bumaga_database,  chunk_size, overlap, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "132f3a71-8d67-439e-bca7-222b3104fd73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:06.156240Z",
     "iopub.status.busy": "2024-11-27T10:13:06.155405Z",
     "iopub.status.idle": "2024-11-27T10:13:06.174898Z",
     "shell.execute_reply": "2024-11-27T10:13:06.173815Z",
     "shell.execute_reply.started": "2024-11-27T10:13:06.156206Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f027a07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:17.912246Z",
     "iopub.status.busy": "2024-11-27T10:13:17.911755Z",
     "iopub.status.idle": "2024-11-27T10:13:18.036645Z",
     "shell.execute_reply": "2024-11-27T10:13:18.035781Z",
     "shell.execute_reply.started": "2024-11-27T10:13:17.912221Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>documents</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>ANSWER_IlyaGusev/saiga_llama3_8b</th>\n",
       "      <th>TIME_IlyaGusev/saiga_llama3_8b</th>\n",
       "      <th>relevant_documents</th>\n",
       "      <th>ANSWER_Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24</th>\n",
       "      <th>TIME_Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Определите масштаб плана, если лес площадью 20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\...</td>\n",
       "      <td>14.534963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\...</td>\n",
       "      <td>27.189879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Определите масштаб карты, если улица длиной 2 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\...</td>\n",
       "      <td>10.992461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\...</td>\n",
       "      <td>17.908840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question documents correct_answer  \\\n",
       "0  Определите масштаб плана, если лес площадью 20...       NaN            NaN   \n",
       "1  Определите масштаб карты, если улица длиной 2 ...       NaN            NaN   \n",
       "\n",
       "                    ANSWER_IlyaGusev/saiga_llama3_8b  \\\n",
       "0  <|start_header_id|>assistant<|end_header_id|>\\...   \n",
       "1  <|start_header_id|>assistant<|end_header_id|>\\...   \n",
       "\n",
       "   TIME_IlyaGusev/saiga_llama3_8b relevant_documents  \\\n",
       "0                       14.534963                NaN   \n",
       "1                       10.992461                NaN   \n",
       "\n",
       "  ANSWER_Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24  \\\n",
       "0  <|start_header_id|>assistant<|end_header_id|>\\...      \n",
       "1  <|start_header_id|>assistant<|end_header_id|>\\...      \n",
       "\n",
       "   TIME_Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24  \n",
       "0                                          27.189879    \n",
       "1                                          17.908840    "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset = pd.read_csv('QA_DATASET_in_progress_with_textbooks.csv')\n",
    "qa_dataset_cols = qa_dataset.columns\n",
    "qa_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbcaa793",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:21.422162Z",
     "iopub.status.busy": "2024-11-27T10:13:21.421728Z",
     "iopub.status.idle": "2024-11-27T10:13:21.493407Z",
     "shell.execute_reply": "2024-11-27T10:13:21.492672Z",
     "shell.execute_reply.started": "2024-11-27T10:13:21.422139Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "QA_DATASET = []\n",
    "for row in qa_dataset.values.tolist():\n",
    "    element = {}\n",
    "    for cn, v in zip(qa_dataset_cols, row):\n",
    "        element[cn] = v\n",
    "    QA_DATASET.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d8f83db-308d-4721-a780-ebd4c1325e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:26.367702Z",
     "iopub.status.busy": "2024-11-27T10:13:26.367288Z",
     "iopub.status.idle": "2024-11-27T10:13:26.379735Z",
     "shell.execute_reply": "2024-11-27T10:13:26.379024Z",
     "shell.execute_reply.started": "2024-11-27T10:13:26.367681Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import nan as np_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e25822a-f722-4a51-a221-1cb8561abee3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:26.905818Z",
     "iopub.status.busy": "2024-11-27T10:13:26.905459Z",
     "iopub.status.idle": "2024-11-27T10:13:26.920280Z",
     "shell.execute_reply": "2024-11-27T10:13:26.919557Z",
     "shell.execute_reply.started": "2024-11-27T10:13:26.905791Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ans_start = '<|start_header_id|>assistant<|end_header_id|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79331a61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:14:24.868945Z",
     "iopub.status.busy": "2024-11-27T10:14:24.868411Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0434cfda3b5b4491bdf3d94ad41164d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/686 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Зачем люди и страны торгуют?\n",
      "Documents after split: 79\n",
      "Documents after processing: 79\n",
      "Embeddings calculated!\n",
      "\tready to retrieve\n",
      "\tstarted gen 0.05795931816101074\n",
      "\tend 40.89516758918762\n",
      "Какие функции выполняют деньги в процессе обмена?\n",
      "Documents after split: 79\n",
      "Documents after processing: 79\n",
      "Embeddings calculated!\n",
      "\tready to retrieve\n",
      "\tstarted gen 0.01745128631591797\n",
      "\tend 19.577024936676025\n",
      "Почему торговлю считают источником экономического благополучия страны?\n",
      "Documents after split: 79\n",
      "Documents after processing: 79\n",
      "Embeddings calculated!\n",
      "\tready to retrieve\n",
      "\tstarted gen 0.01602315902709961\n",
      "\tend 19.889847993850708\n",
      "Для чего нужна реклама товаров и услуг?\n",
      "Documents after split: 79\n",
      "Documents after processing: 79\n",
      "Embeddings calculated!\n",
      "\tready to retrieve\n",
      "\tstarted gen 0.01587510108947754\n",
      "\tend 17.208580493927002\n",
      "Используя знания по курсу истории и материал параграфа, подготовь устный ответ, доказывающий, что появление купечества было значительным событием в развитии цивилизации.\n",
      "Documents after split: 79\n",
      "Documents after processing: 79\n",
      "Embeddings calculated!\n",
      "\tready to retrieve\n",
      "\tstarted gen 0.01642584800720215\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 822.00 MiB. GPU 1 has a total capacity of 23.64 GiB of which 796.56 MiB is free. Including non-PyTorch memory, this process has 22.85 GiB memory in use. Of the allocated memory 20.67 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt}], tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mstarted gen\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time)\n\u001b[0;32m---> 40\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeam_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4090\u001b[39;49m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m answer_decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(answer[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     50\u001b[0m answer_decoded \u001b[38;5;241m=\u001b[39m answer_decoded[answer_decoded\u001b[38;5;241m.\u001b[39mfind(ans_start):]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2246\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2239\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2240\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2241\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2242\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2243\u001b[0m     )\n\u001b[1;32m   2245\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2246\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2252\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2257\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2258\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2259\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2260\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2267\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:3455\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3452\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config())\n\u001b[1;32m   3454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3455\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3457\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3458\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3459\u001b[0m     outputs,\n\u001b[1;32m   3460\u001b[0m     model_kwargs,\n\u001b[1;32m   3461\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3462\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 822.00 MiB. GPU 1 has a total capacity of 23.64 GiB of which 796.56 MiB is free. Including non-PyTorch memory, this process has 22.85 GiB memory in use. Of the allocated memory 20.67 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for triplet in notebook.tqdm(QA_DATASET):\n",
    "    if triplet.get(f'ANSWER_{model_id}') is not np_nan and triplet.get(f'ANSWER_{model_id}') is not None:\n",
    "        continue\n",
    "    question = triplet['question']\n",
    "    print(question)\n",
    "    # print(triplet['documents'])\n",
    "    corpus_index = ''\n",
    "    prompt = ''\n",
    "    if triplet['documents'] == 'metamentor_corpus':\n",
    "        database = deepcopy(metamentor_database)\n",
    "        corpus_index = deepcopy(metamentor_corpus_index)\n",
    "        warn = deepcopy(metamentor_warn)\n",
    "    elif triplet['documents'] == 'svo_corpus':\n",
    "        database = deepcopy(svo_database)\n",
    "        corpus_index = deepcopy(svo_corpus_index)\n",
    "        warn = deepcopy(svo_warn)\n",
    "    elif triplet['documents'] == 'spb_corpus':\n",
    "        database = deepcopy(spb_database)\n",
    "        corpus_index = deepcopy(spb_corpus_index)\n",
    "        warn = deepcopy(spb_warn)\n",
    "    elif triplet['documents'] == 'bumaga_corpus':\n",
    "        database = deepcopy(bumaga_database)\n",
    "        corpus_index = deepcopy(bumaga_corpus_index)\n",
    "        warn = deepcopy(bumaga_warn)\n",
    "    elif isinstance(triplet['documents'], str):\n",
    "        database = []\n",
    "        temp_corpus = upload_files(triplet['documents'])\n",
    "        corpus_index, warn = build_index(temp_corpus, database, chunk_size, overlap, '')\n",
    "    else:\n",
    "        prompt = f\"Ваша задача максимально подробно ответить на вопрос пользователя.\\n\\nВопрос пользователя:\\n{question}\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('\\tready to retrieve')\n",
    "    if corpus_index:     \n",
    "        documents = retrieve(question, corpus_index, '',  k_documents)\n",
    "        triplet['relevant_documents'] = documents\n",
    "    prompt = prompt if prompt else f\"\"\"Ваша задача ответить на вопрос пользователя используя только информацию из предоставленных документов. Отвечайте подробно, но только на основе документов. Если в документах не содержится полезная информация, необходимая для ответа на вопрос, так и скажите. Не пытайтесь вспомнить или придумать ответ самостоятельно.\\n\\nДокументы:\\n{documents}\\n\\nВопрос пользователя:\\n{question}\"\"\"\n",
    "    input_tokens = tokenizer.apply_chat_template([{'role': 'user', 'content': prompt}], tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to('cuda:1')\n",
    "    print('\\tstarted gen', time.time()-start_time)\n",
    "    answer = model.generate(\n",
    "        input_tokens, \n",
    "        do_sample=True,\n",
    "        temperature= temperature,\n",
    "        num_beams= beam_search,\n",
    "        top_k= top_k,\n",
    "        max_new_tokens=4090\n",
    "    )\n",
    "    \n",
    "    answer_decoded = tokenizer.decode(answer[0])\n",
    "    answer_decoded = answer_decoded[answer_decoded.find(ans_start):].strip()\n",
    "    triplet[f'ANSWER_{model_id}'] = answer_decoded\n",
    "    triplet[f'TIME_{model_id}'] = time.time() - start_time\n",
    "    print('\\tend', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5c5464b-425a-4b74-a4b5-8f16d7206868",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:13:28.679059Z",
     "iopub.status.busy": "2024-11-27T10:13:28.678357Z",
     "iopub.status.idle": "2024-11-27T10:13:28.706320Z",
     "shell.execute_reply": "2024-11-27T10:13:28.705030Z",
     "shell.execute_reply.started": "2024-11-27T10:13:28.679037Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09ef63f335c41738ddcf41d136b1c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/686 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for triplet in notebook.tqdm(QA_DATASET):\n",
    "    triplet['relevant_documents'] = triplet.get('relevant_documents')\n",
    "    triplet[f'ANSWER_{model_id}'] = triplet.get(f'ANSWER_{model_id}')\n",
    "    triplet[f'TIME_{model_id}'] = triplet.get(f'TIME_{model_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9ee2bf7-b1ca-437a-bdbb-b8187f010c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:07:52.964380Z",
     "iopub.status.busy": "2024-11-27T10:07:52.964038Z",
     "iopub.status.idle": "2024-11-27T10:07:52.980426Z",
     "shell.execute_reply": "2024-11-27T10:07:52.979660Z",
     "shell.execute_reply.started": "2024-11-27T10:07:52.964360Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(QA_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26568126-7b40-4e7a-bf5a-d872ae2f7c0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:07:53.949587Z",
     "iopub.status.busy": "2024-11-27T10:07:53.949250Z",
     "iopub.status.idle": "2024-11-27T10:07:54.143824Z",
     "shell.execute_reply": "2024-11-27T10:07:54.143032Z",
     "shell.execute_reply.started": "2024-11-27T10:07:53.949567Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('QA_DATASET_in_progress.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fae61736-e593-47e0-ae05-b45cc45c0844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T10:07:58.353889Z",
     "iopub.status.busy": "2024-11-27T10:07:58.353524Z",
     "iopub.status.idle": "2024-11-27T10:07:58.367585Z",
     "shell.execute_reply": "2024-11-27T10:07:58.366883Z",
     "shell.execute_reply.started": "2024-11-27T10:07:58.353862Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'documents', 'correct_answer',\n",
       "       'ANSWER_IlyaGusev/saiga_llama3_8b', 'TIME_IlyaGusev/saiga_llama3_8b',\n",
       "       'relevant_documents'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc4011-5934-4149-94fb-9c95c2e67871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
